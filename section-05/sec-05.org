#+AUTHOR:     
#+TITLE:      
#+OPTIONS:     toc:nil num:nil 
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{dcolumn}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.1,0.2,0.9}}}
#+LATEX: \newcommand{\Rb}{{\bf R}}
#+LATEX: \newcommand{\Rbp}{{\bf R}^{\prime}}
#+LATEX: \newcommand{\Rsq}{R^{2}}
#+LATEX: \newcommand{\ep}{{\bf e}^\prime}
#+LATEX: \renewcommand{\e}{{\bf e}}
#+LATEX: \renewcommand{\b}{{\bf b}}
#+LATEX: \renewcommand{\r}{{\bf r}}
#+LATEX: \renewcommand{\bp}{{\bf b}^{\prime}}
#+LATEX: \renewcommand{\bs}{{\bf b}^{*}}
#+LATEX: \renewcommand{\I}{{\bf I}}
#+LATEX: \renewcommand{\X}{{\bf X}}
#+LATEX: \renewcommand{\M}{{\bf M}}
#+LATEX: \renewcommand{\A}{{\bf A}}
#+LATEX: \renewcommand{\B}{{\bf B}}
#+LATEX: \renewcommand{\C}{{\bf C}}
#+LATEX: \renewcommand{\P}{{\bf P}}
#+LATEX: \renewcommand{\Xp}{{\bf X}^{\prime}}
#+LATEX: \renewcommand{\Xsp}{{\bf X}^{*\prime}}
#+LATEX: \renewcommand{\Xs}{{\bf X}^{*}}
#+LATEX: \renewcommand{\Mp}{{\bf M}^{\prime}}
#+LATEX: \renewcommand{\y}{{\bf y}}
#+LATEX: \renewcommand{\ys}{{\bf y}^{*}}
#+LATEX: \renewcommand{\yp}{{\bf y}^{\prime}}
#+LATEX: \renewcommand{\ysp}{{\bf y}^{*\prime}}
#+LATEX: \renewcommand{\yh}{\hat{{\bf y}}}
#+LATEX: \renewcommand{\yhp}{\hat{{\bf y}}^{\prime}}
#+LATEX: \renewcommand{\In}{{\bf I}_n}
#+LATEX: \renewcommand{\sigs}{\sigma^{2}}
#+LATEX: \renewcommand{\sigsh}{\hat{\sigma}^{2}}
#+LATEX: \renewcommand{\V}{\mathbb{V}}
#+LATEX: \renewcommand{\and}{\mbox{and}}
#+LATEX: \renewcommand{\sumi}{\sum_{i=1}^n}
#+LATEX: \renewcommand{\var}[1]{\textcolor{red}{\mbox{\texttt{#1}}}}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline

*Hypothesis testing* \hfill
*ARE212*: Section 05 \\ \\

This is an introducton to basic hypothesis testing in =R=. We have
shown that, with a certain set of assumptions, the difference between
the OLS estimator and the true parameter vector is distributed
normally as shown in expression (2.63): $$(\b - \beta)|\X \sim N({\bf
0}, \hspace{4pt} \sigs \cdot (\Xp\X)^{-1})$$ We have also shown that
$s^2 = \ep\e/(n-k)$ is an unbiased estimator of $\sigs$ in Section
2.3.4 of the lecture notes. The purpose of the section is not to
rehash the lectures, but instead to use the results to practice
indexing in =R=.

#+begin_src R :results output graphics :exports both :tangle yes :session
  data <- read.csv("../data/auto.csv", header=TRUE)
  names(data) <- c("price", "mpg", "weight")
  y <- matrix(data$price)
  X <- cbind(1, data$mpg, data$weight)
#+end_src

#+RESULTS:

For reference, consider the regression output, using data we've seen
before:

#+begin_src R :results output graphics :exports both :tangle yes :session
res <- lm(price ~ 1 + mpg + weight, data = data)
summary(res)
#+end_src

#+results:
#+begin_example

Call:
lm(formula = price ~ 1 + mpg + weight, data = data)

Residuals:
   Min     1Q Median     3Q    Max 
 -3332  -1858   -504   1256   7507 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)   
(Intercept) 1946.0687  3597.0496   0.541  0.59019   
mpg          -49.5122    86.1560  -0.575  0.56732   
weight         1.7466     0.6414   2.723  0.00813 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 

Residual standard error: 2514 on 71 degrees of freedom
Multiple R-squared: 0.2934,	Adjusted R-squared: 0.2735 
F-statistic: 14.74 on 2 and 71 DF,  p-value: 4.425e-06
#+end_example

We print the full results to examine the $F$-statistic.  Under normal
circumstances, consider just printing =coef(summary(res))=. In order
to perform individual t-tests, we will first have to identify the
standard errors for each coefficient, noting the distribution in
(2.63).  The variance of the error, $\sigs$, can be numerically
estimated, as shown below:

#+begin_src R :results output graphics :exports both :tangle yes :session
  n <- nrow(X); k <- ncol(X)
  P <- X %*% solve(t(X) %*% X) %*% t(X)
  e <- (diag(n) - P) %*% y
  s2 <- t(e) %*% e / (n - k)
  print(s2)
#+end_src

#+RESULTS:
:         [,1]
: [1,] 6320340

The vector of standard errors matches those reported from =R='s
canned routine =lm()=, which is encouraging.

#+begin_src R :results output graphics :exports both :tangle yes :session
  vcov.mat <- as.numeric(s2) * solve(t(X) %*% X)
  se <- sqrt(diag(vcov.mat))
  print(se)
#+end_src

#+RESULTS:
: [1] 3597.0495988   86.1560389    0.6413538

We can now use the vector of standard errors to perform the individual
t-tests.

#+begin_src R :results output graphics :exports both :tangle yes :session
  b <- solve(t(X) %*% X) %*% t(X) %*% y
  apply(b / se, 1, function(t) {2 * pt(-abs(t), df = (n - k))})
#+end_src

#+RESULTS:
: [1] 0.590188628 0.567323727 0.008129813

Great!  We have replicated the =Pr(>|t|)= column of the canned output.
Now let's try to replicate the full regression F-statistic.  This is a
joint test of coefficient significance; are the coefficients jointly
different from a zero vector?  Max has a great description as to why
this is different from three separate tests of significance.  For now,
note that we are testing joint significance by setting
\begin{equation}
\label{eq:fmats}
\Rb = \left[ \begin{array}{ccc} 1 & 0 & 0 \\
                                0 & 1 & 0 \\
                                0 & 0 & 1 \\ \end{array} \right]
\hspace{10pt} \mbox{and} \hspace{10pt}
\r = \left[ \begin{array}{c} 0 \\ 0 \\ 0 \\ \end{array} \right]
\end{equation} 

This is great.  This simplifies the hell out of equation (2.81), which
is fairly daunting at first:

\begin{equation}
\label{eq:F}
F = \frac{(\Rb\b - \r)^{\prime}[\Rb(\Xp\X)^{-1}\Rbp]^{-1}(\Rb\b - \r)/J}{s^2} = 
    \frac{\bp(\Xp\X)\b/J}{s^2}
\end{equation}

#+begin_src R :results output graphics :exports both :tangle yes :session
  F <- t(b) %*% (t(X) %*% X) %*% b / (s2*3)
  print(F)
#+end_src

#+RESULTS:
:          [,1]
: [1,] 158.1714

Well shit.  This is much larger than the reported F-statistic of
14.74.  What happened?  The problem is that we also included the
intercept, whereas =R= assumes that this shouldn't be included in
the joint test.  Simplification failed.  Let's try again.

#+begin_src R :results output graphics :exports both :tangle yes :session
  R <- rbind(c(0, 1, 0), c(0, 0, 1)); J <- 2
  select.var <- solve(R %*% solve(t(X) %*% X) %*% t(R))
  F <- t(R %*% b) %*% select.var %*% (R %*% b) / (s2 * J)
  print(c(F, pf(F, 2, 71, lower.tail=FALSE)))
#+end_src

#+RESULTS:
: [1] 14.7398154  0.9999956

It worked!  And the probability of observing the F-statistic with
degrees of freedom $J=2$ and $n-k = 71$ is printed as well.  

* Puzzle (sort of)

We will replicate the results of a sort of silly study that examines
the causes of McCarthyism, using a /path model/.  The study was
published in the /American Political Science Review/ by Gibson (1988)
and reprinted in Freedman (2009) to illustrate path models, which are
essentially a simple graphical framework to keep track of direct and
indirect causation.  The path model is recreated in Figure
(\ref{fig:path}), and shows that tolerance scores of both the masses
and elites directly impact repression.  This is a theoretical
framework.  The unlabeled connection between the tolerance nodes
indicates association rather than causation.  The repression and
tolerance scores have been standardized, so that they have mean equal
to zero and standard deviation equal to one.\\

The purpose of this exercise is to build up an intuition of the
relationship between the OLS estimates and covariate correlations.  

\begin{figure}[t]
        \centering
        
        \begin{picture}(150,150)(0,0)
        
        \put(0,18){$\var{mass tolerance}$}
        \put(0,127){$\var{elite tolerance}$}
        \put(110,72){$\var{repression}$}

        \put(10,30){\circle*{5}}
        \put(10,120){\circle*{5}}
        \put(100,75){\circle*{5}}

        \thicklines

        \put(10,30){\vector(2,1){87}}
        \put(10,120){\vector(2,-1){87}}

        \thinlines
        
        \qbezier(8,36)(0,75)(8,114)

        \end{picture}

        \caption{Path model, causes of McCarthyism, reproduced from
        Freedman (2009)}

        \label{fig:path}
\end{figure}

Gibson reports the correlation matrix for the path diagram:

|         | Masses | Elite | Repress |
|---------+--------+-------+---------|
| Masses  |   1.00 |  0.52 |   -0.26 |
| Elite   |   0.52 |  1.00 |   -0.42 |
| Repress |  -0.26 | -0.42 |    1.00 |

His model for political repression for $n = 36$ states is given by:
\begin{equation}
\var{repression} = \beta_1 \cdot \var{mass tolerance} + \beta_2 \cdot
\var{elite tolerance} + \epsilon,
\label{eq:one}
\end{equation} Denote $\var{mass tolerance}$ as $M$, $\var{elite
tolerance}$ as $E$, and $\var{repression}$ as $R$, such that Equation
(\ref{eq:one}) becomes $R = \beta_1 M + \beta_2 E + \epsilon$.  Finally,
let $\X = [M \hspace{8pt} E]$, so that $R = \X\beta + \epsilon$.  \\

Here is the kicker.  Since, all  variables were standardized, we know that 
\[
\frac{1}{n} \sum_{i=1}^n E_i = 0 \qquad \and \qquad \frac{1}{n}\sum_{i=1}^n E_i^2 = 1
\]

This is true, also, for $M$ and $R$.  Being careful about matrix
multiplication, we can compute the following:
\begin{equation}
\Xp\X = \left[ 
\begin{array}{cc}
\sumi M_i^2 & \sumi M_i E_i \\
\sumi M_i E_i & \sumi E_i^2  \\
\end{array}\right] = 
n\left[
\begin{array}{cc}
1 & r_{ME} \\
r_{ME} & 1 
\end{array}
\right]=
n\left[
\begin{array}{cc}
1 & 0.52 \\
0.52 & 1 
\end{array}
\right]
\end{equation}

\begin{equation}
\Xp R = \left[ 
\begin{array}{cc}
\sumi M_i R_i  \\
\sumi E_i R_i \\
\end{array}\right] = 
n\left[
\begin{array}{cc}
r_{MR} \\
r_{ER} 
\end{array}
\right]=
n\left[
\begin{array}{cc}
-0.26 \\
-0.42 
\end{array}
\right]
\end{equation}

We don't even need the actual data to compute the OLS coefficients!
Specifically, $\beta = (\Xp\X)^{-1}\Xp R$:

#+begin_src R :results output graphics :exports both :tangle yes :session
  n <- 36
  xtx <- n * matrix(c(1, 0.52, 0.52, 1), ncol = 2)
  xtr <- n * matrix(c(-0.26, -0.42), ncol = 1)
  (b <- solve(xtx) %*% xtr)
#+end_src  

#+RESULTS:
:             [,1]
: [1,] -0.05701754
: [2,] -0.39035088

Given standardized tolerance and repression scores, we can use the
following formula from page 85 in Freedman to calculate the model
variance: $\sigsh = 1 - \hat{\beta}_1^2 - \hat{\beta}_2^2 -
2\hat{\beta}_1\hat{\beta}_2 r_{ME}$

#+begin_src R :results output graphics :exports both :tangle yes :session
  p <- 3
  (sigma.hat.sq <- (n / (n - p)) * (1 - b[1]^2 - b[2]^2 - 2 * b[1] * b[2] * 0.52))
#+end_src  

#+RESULTS:
: [1] 0.8958852

There is an implicit intercept, since the scores are standardized, so
that $p = 3$.  We compute the standard errors from the estimated
covariance matrix, $\sigsh(\Xp\X)^{-1}$.  Note that $\V(\hat{\beta}_k|\X)
= \sigsh(\Xp\X)_{kk}^{-1}$:

#+begin_src R :results output graphics :exports both :tangle yes :session
  vcov.mat <- sigma.hat.sq * solve(xtx)
  se1 <- sqrt(vcov.mat[1,1])
  se2 <- sqrt(vcov.mat[2,2])
  pt(b[1]/se1, n - p)
  pt(b[2]/se2, n - p)
#+end_src  

#+RESULTS:
: [1] 0.3797346
: [1] 0.02109715

The coefficient on $\var{mass tolerance}$ is not significant, but the
coefficient on $\var{elite tolerance}$ is significant.  But are the
two coefficients significantly different from each other?  Let $\Rb =
[1 \hspace{8pt} -1]$ and $\r = [0]$.  Then the following test
statistic will test that the two coefficients are equal.

\begin{equation}
\label{eq:F}
F = \frac{(\Rb \hat{\beta} - \r)^{\prime}[\Rb(\Xp\X)^{-1}\Rbp]^{-1}(\Rb \hat{\beta} - \r)}{\sigsh}
\end{equation}

#+begin_src R :results output graphics :exports both :tangle yes :session
  R <- t(matrix(c(-1, 1))); r <- 0
  G <- R %*% b - r
  (F <- (G %*% R %*% solve(xtx) %*% t(R) %*% t(G))/sigma.hat.sq)
#+end_src  

#+RESULTS:
:            [,1]
: [1,] 0.01435461

The test statistic follows the $F$-distribution, and is not
significant at any reasonable $p$-value.  So, while $\var{elite
tolerance}$ may be significant in the regression of repression on
tolerance, it is not significantly different than the insignificant
variable $\var{mass tolerance}$.
